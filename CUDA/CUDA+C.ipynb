{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating C/C++ code with CUDA on GPUs\n",
    "\n",
    "In this self-paced, hands-on lab, we will use CUDA C/C++ to accelerate code on NVIDIA GPUs.\n",
    "\n",
    "Created by Mark Ebersole @ NVIDIA and Edoardo Baldi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notes on usage\n",
    "\n",
    "- For every step in this guided notebook, there's a corresponding solution in the right folder. A suggested practice would be to copy & rename the original source file, modify it, and the compile it following the hints here. You can always double-check with the solution provided.\n",
    "\n",
    "- To compile & run all the examples in this notebook, you need a working installation of the CUDA Toolkit, that comes with the `nvcc` compiler (and all the rest, including a ton of very useful examples written by NVIDIA developers). If you don't own it, you can find how to download and install it [HERE](https://developer.nvidia.com/cuda-downloads). \n",
    "\n",
    "- **IMPORTANT**: a NVIDIA graphics driver must be installed to be able to **run** properly the examples, if you are not running these on a cluster. In the latter case, everything should be already set up (you may have to load some modules, like `cuda` on Deneb).\n",
    "\n",
    "- **IMPORTANT 2**: to compile & run all the examples, you can either evaluate the cells here in the notebook or, in a terminal, go into the corresponding directory and issue `make`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use one Deneb's GPU node. Each is equipped with a *NVIDIA Tesla K40m (Kepler architecture)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 10:24:02 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GT 730      Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 35%   39C    P0    N/A /  N/A |    270MiB /   971MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0                  Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup previous compiled codes\n",
    "\n",
    "Just go into the root directory (the `sources` directory) and issue `make clean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing and Launching GPU Kernels\n",
    "\n",
    "### Task #1\n",
    "\n",
    "We're going to be accelerating the ever popular SAXPY (**S**ingle-precision **A** times **X** **P**lus **Y**) function on the GPU using CUDA C/C++.\n",
    "\n",
    "Using the concepts introduced in Task #1, modify the following code to run on the GPU.  The `TODO` text in the code will help you focus on the appropriate areas that need modification.  You'll probably notice two new API calls in the code; `cudaMallocManaged` and `cudaFree`.  These two functions are working with managed memory using CUDA's Unified Memory system.  We'll explore this in the last task of this lab.  For the moment, you just need to know that they are replacing `malloc` and `free`, respectively.\n",
    "\n",
    "Open the **task2.cu** file and begin working.  If you get stuck, or just want to check your answer, feel free to look at the **task2_solution.cu** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \r\n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \r\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to compile & run task2.cu\n",
    "!nvcc -arch=sm_35 -o task2_out.x sources/1-writing_kernel/task2.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of your program should be all 5's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "The next function we will accelerate is a basic matrix multiplication function.  In this simplified example, we'll assume our matrices are all square - they have the same number of rows and columns.  Your goal is to modify the `matrixMulGPU` function with CUDA so it will run on the GPU.  However, there is a new twist!  Instead of just using one-dimensional blocks of threads and blocks, we'll be using two dimensions; x and y.  So, in addition to using `blockIdx.x`, `blockDim.x`, and `threadIdx.x`, you'll also need to use <code>blockIdx.<span style=\"color:orange\">**y**</span></code>, <code>blockDim.<span style=\"color:orange\">**y**</span></code>, and <code>threadIdx.<span style=\"color:orange\">**y**</span></code>. \n",
    "\n",
    "Besides replacing the `TODO` blocks in the `matrixMulGPU` function, you will need to finish initializing the `number_of_blocks` variable in the `main` function to launch the appropriate number of thread blocks.  This variable has a `dim3` type; see the [CUDA documentation](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dim3) for more info on this type.  This is all marked with `TODO` in the code.\n",
    "\n",
    "While working on **task3.cu** you can always check the **task3_solution.cu file** to see the final answer or if you get stuck.\n",
    "\n",
    "**Note**: do not modify the CPU version `matrixMulCPU`.  This is used to verify the results of the GPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\r\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to compile & run task3.cu\n",
    "!nvcc -arch=sm_35 -o task3_out.x sources/2-2d_kernel/task3.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "If you started changing the number of blocks and threads per block in the above examples, you may have noticed some cases where you were not getting the expected answer.  Up to this point we have not added any type of error checking, which makes it very hard to tell why a problem is occurring.  Error checking is just as important when programming for a GPU as for a CPU.  So let's add some error checking to the SAXPY example and see if we can introduce some errors to catch.\n",
    "\n",
    "**Note**: In order to focus on a specific topic, and to keep code and instructions short, a number of examples in this lab leave out error checking code.  However, it is highly encouraged that you include error checking in your code whenever possible!\n",
    "\n",
    "### Task #3\n",
    "\n",
    "The SAXPY code for this task is slightly different than in Task #2.  For starters, we are storing the result of our computation back into the `y` array instead of a separate array.  We're also using `float` values for our arrays instead of integers.  On the host side, after calculating our SAXPY result, the code will check for correctness.  If everything is working, the program will print `Max Error: 0.00000`.\n",
    "\n",
    "Now let's discuss the different types of errors we need to check for.  Kernels are launched asynchronously with respect to the host, meaning that control returns to the host code immediately **after** a kernel is launched but **before** the kernel completes. As a result we need to check for two types of errors; synchronous errors dealing with the kernel launch and asynchronous errors that can happen during kernel execution. To check for synchronous errors we use the function `cudaGetLastError()`, which returns an integer error code. In the example below we compare the result of `cudaGetLastError()` and if it does not equal `cudaSuccess` (defined in the `cudaFor` module), we have an error:\n",
    "\n",
    "```cpp\n",
    "cudaError_t ierrSync;\n",
    "...\n",
    "myKernel<<<grid, blocks>>>(...);\n",
    "ierrSync = cudaGetLastError();\n",
    "if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
    "```\n",
    "\n",
    "To check for an error during execution we need to wait for the kernel to complete.  This is accomplished using the function `cudaDeviceSynchronize()`. As with all CUDA API calls, `cudaDeviceSynchronize()` returns an integer error code and used in this fashion will capture any error that occurs on the device while executing the kernel.  Example usage:\n",
    "\n",
    "```cpp\n",
    "cudaError_t ierrAsync;\n",
    "...\n",
    "myKernel<<<grid, blocks>>>(...);\n",
    "ierrAsync = cudaDeviceSynchronize();\n",
    "if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
    "```\n",
    "    \n",
    "In addition to the above code changes, there is a logic bug introduced in the code below.  Your objective for Task #4 is to add both types of error checking to the **task4.cu** source code and try to find the bug.  Before modifying any code, compile and run the program as it is and see the result.  Then add error checking and see if you can figure out what the bug is and fix it.  As always, the solution (**task4_solution.cu**) is provided so you can check your work, or look at if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Error: 0.00000"
     ]
    }
   ],
   "source": [
    "# Execute this cell to compile & run task4.cu\n",
    "!nvcc -arch=sm_35 -o task4_out.x sources/3-errors/task4.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to consistently add error checking into your code.  We routinely get questions regarding problems with someone's code, only to see there is no error checking.  Almost always once error checking is added, the problem becomes obvious and is easily fixed.  \n",
    "\n",
    "## Querying the GPU Device\n",
    "\n",
    "The CUDA C/C++ device management API allows a programmer to query the number of devices available on a system and the features and capabilities of each device. The simple code below illustrates use of the device management API. After the number of CUDA-enabled devices attached to the system is determined via `cudaGetDeviceCount()`, a loop over these devices is performed (note that devices are enumerated from 0) and the function `cudaGetDeviceProperties()` is used to return information about a device in a variable of type `cudaDeviceProp`. \n",
    "\n",
    "```cpp\n",
    "function deviceQuery\n",
    "{\n",
    "  cudaDeviceProp prop;\n",
    "  int nDevices=0, i; \n",
    "  cudaError_t ierr;\n",
    "\n",
    "  ierr = cudaGetDeviceCount(&nDevices);\n",
    " \n",
    "  for( i = 0, i < nDevices; ++i )\n",
    "  {\n",
    "     ierr = cudaGetDeviceProperties(&prop, i);\n",
    "     printf(\"Device number: %d\\n\", i);\n",
    "     printf(\"  Device name: %s\\n\", prop.name);\n",
    "     printf(\"  Compute capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "     printf(\"  Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "     printf(\"  Max threads in X-dimension of block: %d\\n\", prop.maxThreadsDim[0]);\n",
    "     printf(\"  Max threads in Y-dimension of block: %d\\n\", prop.maxThreadsDim[1]);\n",
    "     printf(\"  Max threads in Z-dimension of block: %d\\n\\n\", prop.maxThreadsDim[2]);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The type `cudaDeviceProp` contains many fields.  Only a few are demonstrated in this code. One value demonstrated is the compute capability, which indicates the GPU architecture and is given in `major.minor` format. The major component reflects the generation of the architecture, and the minor component the revision within that generation. All Fermi generation GPUs have a compute capability of 2.x,  Kepler GPUs 3.x, and Maxwell GPUs 5.x. In addition to the compute capability, this code prints the maximum number of threads per block possible as well as the maximum number of threads in each dimension of the block.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "For our next task, let's modify a 2D version of the SAXPY code to check for valid launch configuration arguments before launching the kernel.  The launch configuration parameters to check are the number of threads per block and number of blocks, in both the X and Y dimension.  See the solution if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DEVICE PROPERTIES ==========\r\n",
      "Device number: 0\r\n",
      "  Device name: GeForce GT 730\r\n",
      "  Compute capability: 3.5\r\n",
      "  Max threads per block: 1024\r\n",
      "  Max threads in X-dimension of block: 1024\r\n",
      "  Max threads in Y-dimension of block: 1024\r\n",
      "  Max threads in Z-dimension of block: 64\r\n",
      "\r\n",
      "Max number of threads exceeded!\r\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to compile & run task5.cu\n",
    "!nvcc -arch=sm_35 -o task5_out.x sources/4-device_properties/task5.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try entering different sizes in the block dimension line, `dim3 threads_per_block(32,16,1);`, and make sure your new GPU device property check works correctly!\n",
    "\n",
    "As you begin writing GPU code that could possibly run on multiple or different types of GPUs, you should use the ability to easily query each device to determine the optimal configuration for your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "It is important to realize that the GPU has it's own physical memory; just like the CPU uses system RAM for it's memory.  When executing code on the GPU, we have to ensure any data it needs is first copied across the PCI-Express bus to the GPU's memory before we launch our kernels.\n",
    "\n",
    "Before the release of CUDA 6, it was a requirement that you, the developer, handle allocation and movement of data between the two memory spaces.  This was, and can still be done with the following minimal set of API calls (detailed documentation [here](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY) at [docs.nvidia.com](http://docs.nvidia.com/)):\n",
    "\n",
    "* `cudaMalloc ( void** devPtr, size_t size )` - this API call is used to allocate memory on the GPU, and is very similar to using `malloc` on the CPU.  You provide the address of a pointer that will point to the memory after the call completes successfully, as well as the number of bytes to allocate.\n",
    "* `cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )` - also very similar the standard `memcpy`, this API call is used to copy data between the CPU and GPU.  It takes a destination pointer, a source pointer, the number of bytes to copy, and the fourth parameter indicates which direction the data is traveling: GPU->CPU, CPU->GPU, or GPU->GPU.\n",
    "* `cudaFree ( void* devPtr )` - we use this API call to free any memory we allocated on the GPU.\n",
    "\n",
    "Starting in CUDA 6, this required manual handling of data movement has been removed with the release of Unified Memory.  Unified Memory creates an area of managed memory, and the underlying system handles moving this managed data between CPU and GPU memory spaces when required.  Because of this, getting an application executing on an NVIDIA GPU with CUDA C/C++ has become a much quicker and efficient process - including simpler to maintain code.\n",
    "\n",
    "To make use of this managed memory area with Unified Memory, you use the following API calls.\n",
    "\n",
    "* `cudaMallocManaged ( T** devPtr, size_t size );` - allocate `size` bytes in managed memory and store in devPtr.\n",
    "* `cudaFree ( void* devPtr )` - we use this API call to free any memory we allocated in managed memory.\n",
    "\n",
    "Once you have used `cudaMallocManaged` to allocate some data, you just use the pointer in your code, regardless of whether it's the CPU or GPU accessing the data.  Before Unified Memory, you typically had two pointers associated with data; one for CPU memory and one for GPU memory (usually using the GPU name preceded with a `d_` to indicate device memory).\n",
    "\n",
    "Managed memory is synchronized between memory spaces at kernel launch and any device synchronization points.  This means that on Kepler and Maxwell architectures an explicit synchronization point (typically `cudaDeviceSynchronize()`) needs to be inserted after a kernel launch but before the host will use data that has been generated by that kernel.\n",
    "\n",
    "### Task #5\n",
    "\n",
    "For this task, you need to modify the code in task6.cu so it makes use of Unified Memory instead of the manual data management it currently uses.  If you get stuck, there you can always look to the solution (task6_solution.cu).\n",
    "<pre>On device: name=hello, value=10\n",
    "On host: name=dello, value=11\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: name=hello, value=10\r\n",
      "On host: name=dello, value=11\r\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to compile & run task6.cu\n",
    "!nvcc -arch=sm_35 -o task6_out.x sources/5-memory_management/task6.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see why Unified Memory is so appealing - it removes the requirement for complex data management code.  Allowing you to get your functions executing on the GPU with less developer effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "If you are interested in learning more, you can use the following resources:\n",
    "\n",
    "* Learn more at the [CUDA Developer Zone](https://developer.nvidia.com/category/zone/cuda-zone).\n",
    "* If you have an NVIDIA GPU in your system, you can download and install the [CUDA tookit](https://developer.nvidia.com/cuda-toolkit).\n",
    "* Take the fantastic online and **free** Udacity [Intro to Parallel Programming](https://www.udacity.com/course/cs344) course which uses CUDA C.\n",
    "* Search or ask questions on [Stackoverflow](http://stackoverflow.com/questions/tagged/cuda) using the cuda tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
